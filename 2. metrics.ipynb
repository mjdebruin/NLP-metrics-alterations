{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score\n",
    "from bleurt import score as bleurt_score\n",
    "from textstat import textstat\n",
    "from transformers import logging as transformers_logging\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Let's start with simple n-gram matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple N-gram Analysis:\n",
      "Complex text: The consumption of excessive amounts of carbohydrates may lead to an elevation in blood glucose levels.\n",
      "Simple text: Eating too much sugar can raise blood sugar.\n",
      "Bigram overlap ratio: 0.000\n",
      "\n",
      "Why this is problematic:\n",
      "- Doesn't account for valid simplification operations\n",
      "- Penalizes good simplifications that use different words\n",
      "- No consideration of meaning preservation\n"
     ]
    }
   ],
   "source": [
    "complex_text = \"The consumption of excessive amounts of carbohydrates may lead to an elevation in blood glucose levels.\"\n",
    "simple_text = \"Eating too much sugar can raise blood sugar.\"\n",
    "reference = [\"Eating too many carbs can increase blood sugar levels.\"]\n",
    "\n",
    "def demonstrate_ngram_matching(complex_text, simple_text):\n",
    "    \"\"\"\n",
    "    Demonstrate basic n-gram matching to show its limitations\n",
    "    \"\"\"\n",
    "    # Tokenize texts\n",
    "    complex_tokens = word_tokenize(complex_text.lower())\n",
    "    simple_tokens = word_tokenize(simple_text.lower())\n",
    "    \n",
    "    # Get bigrams\n",
    "    complex_bigrams = set(ngrams(complex_tokens, 2))\n",
    "    simple_bigrams = set(ngrams(simple_tokens, 2))\n",
    "    \n",
    "    # Calculate simple overlap\n",
    "    overlap = len(complex_bigrams & simple_bigrams)\n",
    "    total = len(complex_bigrams | simple_bigrams)\n",
    "    \n",
    "    print(\"Simple N-gram Analysis:\")\n",
    "    print(f\"Complex text: {complex_text}\")\n",
    "    print(f\"Simple text: {simple_text}\")\n",
    "    print(f\"Bigram overlap ratio: {overlap/total:.3f}\")\n",
    "    print(\"\\nWhy this is problematic:\")\n",
    "    print(\"- Doesn't account for valid simplification operations\")\n",
    "    print(\"- Penalizes good simplifications that use different words\")\n",
    "    print(\"- No consideration of meaning preservation\")\n",
    "\n",
    "demonstrate_ngram_matching(complex_text, simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Moving to BLEU - a more sophisticated n-gram based metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score Analysis:\n",
      "BLEU-1: 0.091\n",
      "BLEU-2: 0.022\n",
      "\n",
      "Limitations for Simplification:\n",
      "- Designed for translation, not simplification\n",
      "- Expects high n-gram overlap\n",
      "- Can't handle valid structural changes\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleu(complex_text, simple_text):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score to show its limitations for simplification\n",
    "    \"\"\"\n",
    "    reference = word_tokenize(complex_text.lower())\n",
    "    candidate = word_tokenize(simple_text.lower())\n",
    "    \n",
    "    # Calculate BLEU with different n-gram settings\n",
    "    bleu_1 = sentence_bleu([reference], candidate, weights=(1, 0, 0, 0),\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "    bleu_2 = sentence_bleu([reference], candidate, weights=(0.5, 0.5, 0, 0),\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "    \n",
    "    print(\"\\nBLEU Score Analysis:\")\n",
    "    print(f\"BLEU-1: {bleu_1:.3f}\")\n",
    "    print(f\"BLEU-2: {bleu_2:.3f}\")\n",
    "    print(\"\\nLimitations for Simplification:\")\n",
    "    print(\"- Designed for translation, not simplification\")\n",
    "    print(\"- Expects high n-gram overlap\")\n",
    "    print(\"- Can't handle valid structural changes\")\n",
    "\n",
    "calculate_bleu(complex_text, simple_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Introducing SARI - designed specifically for simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SARI Analysis:\n",
      "SARI Score: 0.441\n",
      "Add F1: 0.000 (captures good additions)\n",
      "Keep F1: 0.400 (captures preserving important content)\n",
      "Delete F1: 0.923 (captures good deletions)\n",
      "\n",
      "Advantages:\n",
      "- Designed specifically for simplification\n",
      "- Considers addition, deletion, and keeping operations\n",
      "- Better aligned with simplification goals\n"
     ]
    }
   ],
   "source": [
    "def calculate_f1(matches, system_count, reference_count):\n",
    "    # Calculate F1 score by comparing system output against reference matches\n",
    "    if system_count + reference_count == 0:  # Nothing to compare\n",
    "        return 1.0\n",
    "    return 2 * matches / (system_count + reference_count)\n",
    "\n",
    "def calculate_sari(orig, simp, refs):\n",
    "    # Convert to word sets\n",
    "    orig_words = set(word_tokenize(orig.lower()))\n",
    "    simp_words = set(word_tokenize(simp.lower()))\n",
    "    ref_words = [set(word_tokenize(ref.lower())) for ref in refs]\n",
    "    \n",
    "    # Words that were added \n",
    "    system_added = simp_words - orig_words\n",
    "    reference_added = set.union(*[ref - orig_words for ref in ref_words])\n",
    "    add_score = calculate_f1(\n",
    "        len(system_added & reference_added),  # Correct additions\n",
    "        len(system_added),                    # System additions\n",
    "        len(reference_added)                  # Reference additions\n",
    "    )\n",
    "    \n",
    "    # Words that were kept\n",
    "    system_kept = orig_words & simp_words  \n",
    "    reference_kept = set.union(*[orig_words & ref for ref in ref_words])\n",
    "    keep_score = calculate_f1(\n",
    "        len(system_kept & reference_kept),\n",
    "        len(system_kept),\n",
    "        len(reference_kept)\n",
    "    )\n",
    "    \n",
    "    # Words that were deleted\n",
    "    system_deleted = orig_words - simp_words\n",
    "    reference_deleted = set.union(*[orig_words - ref for ref in ref_words])\n",
    "    delete_score = calculate_f1(\n",
    "        len(system_deleted & reference_deleted),\n",
    "        len(system_deleted),\n",
    "        len(reference_deleted)\n",
    "    )\n",
    "    \n",
    "    # Average the three scores\n",
    "    sari = (add_score + keep_score + delete_score) / 3.0\n",
    "    return sari, add_score, keep_score, delete_score\n",
    "\n",
    "# Let's see how SARI evaluates our example\n",
    "sari, add_f1, keep_f1, del_f1 = calculate_sari(complex_text, simple_text, reference)\n",
    "print(\"\\nSARI Analysis:\")\n",
    "print(f\"SARI Score: {sari:.3f}\")\n",
    "print(f\"Add F1: {add_f1:.3f} (captures good additions)\")\n",
    "print(f\"Keep F1: {keep_f1:.3f} (captures preserving important content)\")\n",
    "print(f\"Delete F1: {del_f1:.3f} (captures good deletions)\")\n",
    "print(\"\\nAdvantages:\")\n",
    "print(\"- Designed specifically for simplification\")\n",
    "print(\"- Considers addition, deletion, and keeping operations\")\n",
    "print(\"- Better aligned with simplification goals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Modern approach - BERTScore for semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERTScore Analysis:\n",
      "Precision: 0.931 (How accurate is the simplified content?)\n",
      "Recall: 0.910 (How much original meaning is retained?)\n",
      "F1: 0.920 (Overall semantic similarity)\n",
      "\n",
      "What these scores mean for simplification:\n",
      "- High precision: simplified text is accurate/faithful\n",
      "- High recall: maintains important information from original\n",
      "- High F1: good balance between simplicity and meaning preservation\n",
      "\n",
      "Advantages:\n",
      "- Captures semantic similarity at word/phrase level\n",
      "- Less dependent on exact word matches\n",
      "- Can handle paraphrasing and restructuring\n"
     ]
    }
   ],
   "source": [
    "def calculate_bertscore(complex_text, simple_text):\n",
    "    \"\"\"\n",
    "    Calculate BERTScore components for semantic similarity.\n",
    "\n",
    "    Args:\n",
    "        complex_text (str): The original complex text.\n",
    "        simple_text (str): The simplified text.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Precision (P), Recall (R), and F1 score as floats.\n",
    "    \"\"\"\n",
    "    P, R, F1 = bert_score([simple_text], [complex_text], lang='en', verbose=False)\n",
    "\n",
    "    return P[0].item(), R[0].item(), F1[0].item()\n",
    "\n",
    "# Let's see how BERTScore evaluates our example\n",
    "precision, recall, f1 = calculate_bertscore(complex_text, simple_text)\n",
    "print(\"\\nBERTScore Analysis:\")\n",
    "print(f\"Precision: {precision:.3f} (How accurate is the simplified content?)\")\n",
    "print(f\"Recall: {recall:.3f} (How much original meaning is retained?)\")\n",
    "print(f\"F1: {f1:.3f} (Overall semantic similarity)\")\n",
    "print(\"\\nWhat these scores mean for simplification:\")\n",
    "print(\"- High precision: simplified text is accurate/faithful\")\n",
    "print(\"- High recall: maintains important information from original\")\n",
    "print(\"- High F1: good balance between simplicity and meaning preservation\")\n",
    "print(\"\\nAdvantages:\")\n",
    "print(\"- Captures semantic similarity at word/phrase level\")\n",
    "print(\"- Less dependent on exact word matches\")\n",
    "print(\"- Can handle paraphrasing and restructuring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When running BERTScore, you'll see a warning about RoBERTa model weights.\n",
    "This warning appears because BERTScore uses the RoBERTa model for semantic similarity, but doesn't need the pooler layer that's typically used for classification tasks.\n",
    "The warning is normal and doesn't affect our similarity calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: BLEURT - Trained with human judgments\n",
    "\n",
    "You need to point to the downloaded checkpoint directory\n",
    "Use the path to where you extracted the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEURT Score: 0.708\n"
     ]
    }
   ],
   "source": [
    "def calculate_bleurt(complex_text, simple_text):\n",
    "    \"\"\"\n",
    "    Calculate BLEURT score - learned metric based on human judgments\n",
    "    \"\"\"\n",
    "    checkpoint = \"BLEURT-20\"\n",
    "    scorer = bleurt_score.BleurtScorer(checkpoint)\n",
    "    scores = scorer.score(references=[complex_text], candidates=[simple_text])\n",
    "    return scores[0]\n",
    "\n",
    "bleurt_scores = calculate_bleurt(complex_text, simple_text)\n",
    "print(f\"\\nBLEURT Score: {bleurt_scores:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: G-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will require an OpenAI API-key, which you can fill in below in the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"Your-key-here\"\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score: 5\n",
      "Coherence Score: 5\n",
      "Consistency Score: 5\n",
      "Fluency Score: 5\n"
     ]
    }
   ],
   "source": [
    "def calculate_geval(complex_text, simple_text, metric=\"Relevance\"):\n",
    "    \"\"\"\n",
    "    Calculate G-eval score for a single metric (Relevance, Coherence, Consistency, or Fluency)\n",
    "    Args:\n",
    "        complex_text: original complex sentence\n",
    "        simple_text: simplified version\n",
    "        metric: which metric to evaluate (default: Relevance)\n",
    "    Returns:\n",
    "        score: integer between 1-5\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Metric definitions\n",
    "    metrics = {\n",
    "        \"Relevance\": (\n",
    "            \"Relevance(1-5) - How well the simplified version maintains the important content from the complex sentence.\",\n",
    "            \"1. Read both versions carefully.\\n2. Check if all key information is preserved.\\n3. Assign score 1-5 (1:most info missing, 5:all info preserved)\"\n",
    "        ),\n",
    "        \"Coherence\": (\n",
    "            \"Coherence(1-5) - How well-structured and logical the simplified sentence is.\",\n",
    "            \"1. Read both versions carefully.\\n2. Check sentence structure and flow.\\n3. Assign score 1-5 (1:very poor structure, 5:perfectly clear)\"\n",
    "        ),\n",
    "        \"Consistency\": (\n",
    "            \"Consistency(1-5) - The factual alignment between the complex and simplified sentences.\",\n",
    "            \"1. Read both versions carefully.\\n2. Check for factual errors.\\n3. Assign score 1-5 (1:major errors, 5:perfect alignment)\"\n",
    "        ),\n",
    "        \"Fluency\": (\n",
    "            \"Fluency(1-5) - The linguistic quality in terms of grammar, clarity, and readability.\",\n",
    "            \"1. Read the simplified version.\\n2. Check grammar and readability.\\n3. Assign score 1-5 (1:poor grammar, 5:perfectly formed)\"\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    criteria, steps = metrics[metric]\n",
    "    \n",
    "    prompt = f\"\"\"You will be given an original complex sentence and a simplified version. Rate the simplified version on one metric.\n",
    "\n",
    "Evaluation Criteria:\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "{steps}\n",
    "\n",
    "Source Text:\n",
    "{complex_text}\n",
    "\n",
    "Summary:\n",
    "{simple_text}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "- {metric}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5\n",
    "    )\n",
    "    \n",
    "    # Extract and clean score\n",
    "    score = response.choices[0].message.content.replace(':', '').strip()\n",
    "    score = int(''.join(filter(str.isdigit, score)))\n",
    "    return max(1, min(5, score))  # Ensure score is between 1-5\n",
    "\n",
    "# Example usage:\n",
    "complex_text = \"The implementation of the Affordable Care Act, commonly known as Obamacare, has led to significant changes in the healthcare system of the United States.\"\n",
    "simple_text = \"Obamacare has changed US healthcare a lot.\"\n",
    "\n",
    "for metric in [\"Relevance\", \"Coherence\", \"Consistency\", \"Fluency\"]:\n",
    "    score = calculate_geval(complex_text, simple_text, metric)\n",
    "    print(f\"{metric} Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's apply all metrics to our actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel_file(file_path):\n",
    "    \"\"\"\n",
    "    Process Excel file and compute metrics with sentence tracking:\n",
    "    - SARI (with add, keep, delete components)\n",
    "    - BLEU (bigram-based)\n",
    "    - BERTScore (precision, recall, F1)\n",
    "    - BLEURT (learned metric)\n",
    "    \"\"\"\n",
    "    # Initialize BLEURT scorer (do this once, outside the loop)\n",
    "    checkpoint = \"BLEURT-20\"\n",
    "    bleurt_scorer = bleurt_score.BleurtScorer(checkpoint)\n",
    "    df = pd.read_excel(file_path)\n",
    "    results = []\n",
    "\n",
    "    # Silence the weight initialization warning.\n",
    "    transformers_logging.set_verbosity(transformers_logging.CRITICAL)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Ensure generated is the first version\n",
    "        versions = {\n",
    "            'generated': row['generated_simple'],\n",
    "            'fact_reversal': row['fact_reversal'],\n",
    "            'info_omission': row['info_omission'],\n",
    "            'unsupported_info': row['unsupported_info'],\n",
    "            'subject_object_reversal': row['subject_object_reversal'],\n",
    "            'partial_meaning': row['partial_meaning']\n",
    "        }\n",
    "        \n",
    "        for version_name, current_text in versions.items():\n",
    "            # Calculate all metrics\n",
    "            sari, add_f1, keep_f1, del_f1 = calculate_sari(\n",
    "                row['complex'],\n",
    "                current_text,\n",
    "                [row['original_simple']]\n",
    "            )\n",
    "            \n",
    "            # 2. BLEU score (bigram-based)\n",
    "            bleu = sentence_bleu(\n",
    "                [word_tokenize(row['original_simple'])],\n",
    "                word_tokenize(current_text),\n",
    "                weights=(0.5, 0.5),\n",
    "                smoothing_function=SmoothingFunction().method1\n",
    "            )\n",
    "            \n",
    "            # 3. BERTScore components\n",
    "            P, R, F1 = bert_score([current_text], [row['original_simple']], lang='en', verbose=False)\n",
    "            bert_precision, bert_recall, bert_f1 = P.numpy()[0], R.numpy()[0], F1.numpy()[0]\n",
    "            \n",
    "            # 4. BLEURT score\n",
    "            bleurt = bleurt_scorer.score(\n",
    "                references=[row['original_simple']],\n",
    "                candidates=[current_text]\n",
    "            )[0]\n",
    "            \n",
    "            results.append({\n",
    "                'sentence_id': idx + 1,\n",
    "                'version': version_name,\n",
    "                'current_version_text': current_text,  # The text of whichever version we're currently evaluating\n",
    "                'complex': row['complex'],            # Original complex sentence\n",
    "                'original_simple': row['original_simple'],  # The human reference simplification\n",
    "                'generated_simple': row['generated_simple'],  # The model's output\n",
    "                'sari': sari,\n",
    "                'add_f1': add_f1,\n",
    "                'keep_f1': keep_f1,\n",
    "                'del_f1': del_f1,\n",
    "                'bleu': bleu,\n",
    "                'bertscore_precision': bert_precision,\n",
    "                'bertscore_recall': bert_recall,\n",
    "                'bertscore_f1': bert_f1,\n",
    "                'bleurt': bleurt\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to Excel file with formatting\n",
    "    results_df.to_excel('metric_results.xlsx', \n",
    "                       sheet_name='Metrics Analysis',\n",
    "                       index=False,\n",
    "                       float_format='%.4f')\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scores by version:\n",
      "                         sari  add_f1  keep_f1  del_f1  bleu  \\\n",
      "version                                                        \n",
      "fact_reversal           0.490   0.168    0.735   0.566 0.340   \n",
      "generated               0.505   0.180    0.755   0.579 0.383   \n",
      "info_omission           0.430   0.135    0.602   0.554 0.240   \n",
      "partial_meaning         0.477   0.122    0.744   0.566 0.295   \n",
      "subject_object_reversal 0.490   0.164    0.734   0.573 0.305   \n",
      "unsupported_info        0.492   0.120    0.768   0.587 0.290   \n",
      "\n",
      "                         bertscore_precision  bertscore_recall  bertscore_f1  \\\n",
      "version                                                                        \n",
      "fact_reversal                          0.932             0.938         0.935   \n",
      "generated                              0.943             0.948         0.946   \n",
      "info_omission                          0.945             0.914         0.929   \n",
      "partial_meaning                        0.924             0.942         0.932   \n",
      "subject_object_reversal                0.918             0.924         0.921   \n",
      "unsupported_info                       0.919             0.944         0.931   \n",
      "\n",
      "                         bleurt  flesch_reading_ease  flesch_kincaid_grade  \n",
      "version                                                                     \n",
      "fact_reversal             0.681               60.145                 9.415  \n",
      "generated                 0.743               62.394                 8.955  \n",
      "info_omission             0.644               65.303                 7.640  \n",
      "partial_meaning           0.683               58.967                 9.030  \n",
      "subject_object_reversal   0.662               62.792                 9.225  \n",
      "unsupported_info          0.682               59.035                 9.420  \n"
     ]
    }
   ],
   "source": [
    "results = process_excel_file('results_with_alterations.xlsx')\n",
    "numeric_columns = results.select_dtypes(include=['float64', 'float32', 'int64']).columns.drop('sentence_id')\n",
    "print(\"\\nAverage scores by version:\")\n",
    "print(results.groupby('version')[numeric_columns].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given an original complex sentence and a simplified version of that sentence. Your task is to rate the simplified version on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this document open while reviewing, and refer to it as needed.\n",
    "Please be very critical in your evaluation.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{document}\n",
    "\n",
    "Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "# Metric definitions\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "Relevance(1-5) - How well the simplified version maintains the important content from the complex sentence. \\\n",
    "The simplified version should preserve all key information from the complex sentence. \\\n",
    "Penalize versions that omit crucial information or add information not present in the original sentence.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read both the complex sentence and its simplified version carefully.\n",
    "2. Identify the key information points in the complex sentence.\n",
    "3. Check if all key information is preserved in the simplified version.\n",
    "4. Assign a relevance score from 1 to 5, where:\n",
    "   1: Most key information is missing\n",
    "   2: Significant information is lost\n",
    "   3: Some important information is missing\n",
    "   4: Most key information is preserved\n",
    "   5: All key information is perfectly preserved\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\"\n",
    "Coherence(1-5) - How well-structured and logical the simplified sentence is. \\\n",
    "The simplified version should present information in a clear, natural order. \\\n",
    "The sentence should flow well and maintain logical connections between ideas.\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "1. Read both versions carefully.\n",
    "2. Evaluate how well the simplified version organizes the information.\n",
    "3. Check if the logical relationships between ideas are maintained.\n",
    "4. Assign a coherence score from 1 to 5, where:\n",
    "   1: Extremely difficult to follow\n",
    "   2: Poor organization of ideas\n",
    "   3: Somewhat clear but could be better structured\n",
    "   4: Well-structured with minor issues\n",
    "   5: Perfectly clear and logically structured\n",
    "\"\"\"\n",
    "\n",
    "CONSISTENCY_SCORE_CRITERIA = \"\"\"\n",
    "Consistency(1-5) - The factual alignment between the complex and simplified sentences. \\\n",
    "A consistent simplification contains only statements that are entailed by the complex sentence. \\\n",
    "Penalize any alterations that change the meaning or introduce incorrect facts.\n",
    "\"\"\"\n",
    "\n",
    "CONSISTENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read both versions carefully.\n",
    "2. Check for any factual changes or contradictions.\n",
    "3. Verify that all stated relationships and facts match the complex sentence.\n",
    "4. Assign a consistency score from 1 to 5, where:\n",
    "   1: Major factual errors or contradictions\n",
    "   2: Significant meaning changes\n",
    "   3: Minor factual discrepancies\n",
    "   4: Mostly factually accurate with tiny imprecisions\n",
    "   5: Perfect factual alignment\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_CRITERIA = \"\"\"\n",
    "Fluency(1-5) - The linguistic quality of the simplified sentence in terms of grammar, clarity, and readability.\n",
    "1: Poor grammar or structure making it hard to understand\n",
    "2: Notable grammatical issues but main point is comprehensible\n",
    "3: Generally correct grammar with some awkward phrasing\n",
    "4: Clear and well-written with minimal issues\n",
    "5: Perfectly formed, clear, and natural-sounding sentence\n",
    "\"\"\"\n",
    "\n",
    "FLUENCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the simplified version carefully.\n",
    "2. Evaluate grammar, word choice, and sentence structure.\n",
    "3. Consider how natural and easy to read the sentence is.\n",
    "4. Assign a fluency score from 1 to 5 based on the criteria above.\n",
    "\"\"\"\n",
    "\n",
    "def get_geval_score(criteria: str, steps: str, document: str, summary: str, metric_name: str):\n",
    "    \"\"\"Get evaluation score for a single metric\"\"\"\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        metric_name=metric_name,\n",
    "        document=document,\n",
    "        summary=summary,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # Using GPT-4 for evaluation\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def add_geval_scores(results_df):\n",
    "    \"\"\"\n",
    "    Add G-eval scores to existing results DataFrame\n",
    "    \"\"\"\n",
    "    evaluation_metrics = {\n",
    "        \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "        \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "        \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n",
    "        \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n",
    "    }\n",
    "    \n",
    "    enhanced_df = results_df.copy()\n",
    "    \n",
    "    # Add G-eval scores for each row\n",
    "    for idx, row in enhanced_df.iterrows():\n",
    "        # Get scores for each metric\n",
    "        for metric_name, (criteria, steps) in evaluation_metrics.items():\n",
    "            try:\n",
    "                score = get_geval_score(\n",
    "                    criteria=criteria,\n",
    "                    steps=steps,\n",
    "                    document=row['complex'],\n",
    "                    summary=row['current_version_text'],\n",
    "                    metric_name=metric_name\n",
    "                )\n",
    "                # Clean up the response - remove any colons and extra spaces\n",
    "                score = score.replace(':', '').strip()\n",
    "                # Extract just the number\n",
    "                score_value = ''.join(filter(str.isdigit, score))\n",
    "                # Convert to integer\n",
    "                score_value = int(score_value)\n",
    "                # Ensure score is within 1-5 range\n",
    "                score_value = max(1, min(5, score_value))\n",
    "                enhanced_df.at[idx, f'geval_{metric_name.lower()}'] = score_value\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Invalid score format for {metric_name} at sentence {row['sentence_id']}, version {row['version']}: {score}\")\n",
    "                enhanced_df.at[idx, f'geval_{metric_name.lower()}'] = 3\n",
    "    \n",
    "    return enhanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_results = add_geval_scores(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_results.to_excel('final_evaluation_results.xlsx', \n",
    "                        sheet_name='Complete Analysis',\n",
    "                        index=False,\n",
    "                        float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average scores by version:\n",
      "                         sari  add_f1  keep_f1  del_f1  bleu  \\\n",
      "version                                                        \n",
      "fact_reversal           0.490   0.168    0.736   0.566 0.340   \n",
      "generated               0.505   0.180    0.755   0.579 0.383   \n",
      "info_omission           0.430   0.135    0.602   0.554 0.240   \n",
      "partial_meaning         0.477   0.122    0.744   0.566 0.295   \n",
      "subject_object_reversal 0.490   0.164    0.734   0.573 0.305   \n",
      "unsupported_info        0.492   0.120    0.768   0.587 0.290   \n",
      "\n",
      "                         bertscore_precision  bertscore_recall  bertscore_f1  \\\n",
      "version                                                                        \n",
      "fact_reversal                          0.932             0.938         0.935   \n",
      "generated                              0.943             0.948         0.946   \n",
      "info_omission                          0.945             0.914         0.929   \n",
      "partial_meaning                        0.924             0.942         0.933   \n",
      "subject_object_reversal                0.918             0.924         0.921   \n",
      "unsupported_info                       0.919             0.944         0.931   \n",
      "\n",
      "                         bleurt  flesch_reading_ease  flesch_kincaid_grade  \\\n",
      "version                                                                      \n",
      "fact_reversal             0.681               60.145                 9.415   \n",
      "generated                 0.743               62.394                 8.955   \n",
      "info_omission             0.644               65.303                 7.640   \n",
      "partial_meaning           0.683               58.967                 9.030   \n",
      "subject_object_reversal   0.662               62.792                 9.225   \n",
      "unsupported_info          0.682               59.035                 9.420   \n",
      "\n",
      "                         geval_relevance  geval_coherence  geval_consistency  \\\n",
      "version                                                                        \n",
      "fact_reversal                      1.250            1.500              1.050   \n",
      "generated                          5.000            5.000              5.000   \n",
      "info_omission                      4.050            4.500              4.600   \n",
      "partial_meaning                    3.400            4.100              3.000   \n",
      "subject_object_reversal            4.650            4.200              4.500   \n",
      "unsupported_info                   2.250            3.450              2.150   \n",
      "\n",
      "                         geval_fluency  \n",
      "version                                 \n",
      "fact_reversal                    1.850  \n",
      "generated                        5.000  \n",
      "info_omission                    4.700  \n",
      "partial_meaning                  4.300  \n",
      "subject_object_reversal          4.150  \n",
      "unsupported_info                 4.100  \n"
     ]
    }
   ],
   "source": [
    "# Get all numeric columns (excluding sentence_id)\n",
    "numeric_columns = [col for col in enhanced_results.select_dtypes(include=['float64', 'float32', 'int64']).columns \n",
    "                  if col != 'sentence_id']\n",
    "\n",
    "# Calculate and print means for each version\n",
    "print(\"\\nAverage scores by version:\")\n",
    "print(enhanced_results.groupby('version')[numeric_columns].mean().round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
